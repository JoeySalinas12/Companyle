{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "1. Gather a list of each company in the S&P 500\n",
    "   1. From Wikipedia\n",
    "      1. Company Name\n",
    "      2. Ticker\n",
    "      3. Headquarters Location\n",
    "      4. GCIS Sub-Industry\n",
    "      5. Year Founded\n",
    "   2. From "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a page exists\n",
    "wiki_wiki = wikipediaapi.Wikipedia('Joeys company guesser project', 'en')\n",
    "\n",
    "# Save the contents of the page within a variable\n",
    "page_py = wiki_wiki.page('List of S&P 500 companies')\n",
    "\n",
    "# Save the contents of the page as text\n",
    "page_text = page_py.text\n",
    "\n",
    "def print_sections(sections, level=0):\n",
    "    \"\"\" Gather all of the top level sections within a particular wikipedia page\n",
    "\n",
    "    Args:\n",
    "        sections (str): _description_\n",
    "        level (int, optional): _description_. Defaults to 0.\n",
    "    \"\"\"\n",
    "    for s in sections:\n",
    "        # if \"S&P 500 component stocks\" in s.title:\n",
    "            # print(s.title)\n",
    "            # print(s.text[0:200])\n",
    "        print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:4000]))\n",
    "        print_sections(s.sections, level + 1)\n",
    "print_sections(page_py.sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Page - Title: %s\" % page_py.title)\n",
    "\n",
    "# print(\"Page - Summary: %s\" % page_py.summary[0:6090])\n",
    "# print(page_py.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='Joeys company guesser project',\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Get the page's HTML content\n",
    "URL = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "response = requests.get(URL)\n",
    "\n",
    "# Convert the table to a Pandas DataFrame\n",
    "df = pd.read_html(response.content, flavor='lxml')[0]\n",
    "\n",
    "# Display the first few rows of the table\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find random company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "N = len(df)\n",
    "for i in range(100):\n",
    "    pick_random_company = random.randint(0,N-1)\n",
    "    if pick_random_company > 500:\n",
    "        print(pick_random_company)\n",
    "        \n",
    "print(pick_random_company)\n",
    "display(df.iloc[pick_random_company])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get wiki link for each company & append it to the dataframe in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "first_row = table.find_all('tr')[0]\n",
    "first_link = first_row.find_all('th')[2]\n",
    "\n",
    "# # print(first_row)\n",
    "# count = 0\n",
    "# for row in table.find_all('tr')[1:2]:\n",
    "#     print(row)\n",
    "#     print(count)\n",
    "#     count += 1\n",
    "\n",
    "# Find all of the links in the second column of the table\n",
    "links = []\n",
    "for row in table.find_all('tr')[1:]: # skip the header row\n",
    "    cells = row.find_all('td')\n",
    "    if cells:\n",
    "        link_tag = cells[1].find('a')  # Adjust the index based on the column containing the link\n",
    "        if link_tag and 'href' in link_tag.attrs:\n",
    "            links.append(link_tag['href'])\n",
    "\n",
    "# Create column in existing dataframe with link as element, but with full url\n",
    "updated_df = df.assign(link=links)\n",
    "updated_df['wiki_link'] = 'https://en.wikipedia.org' + updated_df['link']\n",
    "display(updated_df)\n",
    "\n",
    "# save the dataframe to a CSV file\n",
    "updated_df.to_csv('S&P_500_companies.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_revenue_from_page(sp500_df):\n",
    "\n",
    "    # loop through the dataframe and get the revenue for each company\n",
    "    for index, row in sp500_df.iterrows():\n",
    "        print(index,row)\n",
    "        # # Get the HTML content of the linked page\n",
    "        # link = updated_df.iloc[index]['wiki_link']\n",
    "        # page_response = requests.get(link)\n",
    "        # page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "\n",
    "        # # find the table that include information regarding revenue\n",
    "        # table = page_soup.find_all('table', {'class': 'infobox ib-company vcard'})\n",
    "        # for row in table:\n",
    "        #     revenue = row.find_all('tr')[12]\n",
    "        #     revenue2 = revenue.find_all('span')[2].text\n",
    "        #     span_with_title = revenue.find('span').find('span', title=True)\n",
    "        #     # Extract the title attribute\n",
    "        #     title = span_with_title['title'] if span_with_title else None\n",
    "        #     print(title, revenue2[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_revenue_from_page(updated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
