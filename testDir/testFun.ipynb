{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "1. Gather a list of each company in the S&P 500\n",
    "   1. From Wikipedia\n",
    "      1. Company Name\n",
    "      2. Ticker\n",
    "      3. Headquarters Location\n",
    "      4. GCIS Sub-Industry\n",
    "      5. Year Founded\n",
    "   2. From "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a page exists\n",
    "wiki_wiki = wikipediaapi.Wikipedia('Joeys company guesser project', 'en')\n",
    "\n",
    "# Save the contents of the page within a variable\n",
    "page_py = wiki_wiki.page('List of S&P 500 companies')\n",
    "\n",
    "# Save the contents of the page as text\n",
    "page_text = page_py.text\n",
    "\n",
    "def print_sections(sections, level=0):\n",
    "    \"\"\" Gather all of the top level sections within a particular wikipedia page\n",
    "\n",
    "    Args:\n",
    "        sections (str): _description_\n",
    "        level (int, optional): _description_. Defaults to 0.\n",
    "    \"\"\"\n",
    "    for s in sections:\n",
    "        # if \"S&P 500 component stocks\" in s.title:\n",
    "            # print(s.title)\n",
    "            # print(s.text[0:200])\n",
    "        print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:4000]))\n",
    "        print_sections(s.sections, level + 1)\n",
    "print_sections(page_py.sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Page - Title: %s\" % page_py.title)\n",
    "\n",
    "# print(\"Page - Summary: %s\" % page_py.summary[0:6090])\n",
    "# print(page_py.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='Joeys company guesser project',\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Get the page's HTML content\n",
    "URL = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "response = requests.get(URL)\n",
    "\n",
    "# Convert the table to a Pandas DataFrame\n",
    "df = pd.read_html(response.content, flavor='lxml')[0]\n",
    "\n",
    "# Display the first few rows of the table\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "N = len(df)\n",
    "for i in range(100):\n",
    "    pick_random_company = random.randint(0,N-1)\n",
    "    if pick_random_company > 500:\n",
    "        print(pick_random_company)\n",
    "        \n",
    "print(pick_random_company)\n",
    "display(df.iloc[pick_random_company])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newURL = \n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "\n",
    "# Find all of the links in the second column of the table\n",
    "links = []\n",
    "for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "    cells = row.find_all('td')\n",
    "    if cells:\n",
    "        link_tag = cells[1].find('a')  # Adjust the index based on the column containing the link\n",
    "        if link_tag and 'href' in link_tag.attrs:\n",
    "            links.append(link_tag['href'])\n",
    "\n",
    "# Extrant html info from the linked page\n",
    "full_url = f'https://en.wikipedia.org{link}'  # Construct the full URL\n",
    "page_response = requests.get(full_url)\n",
    "page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
